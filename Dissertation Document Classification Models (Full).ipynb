{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "126efb20",
   "metadata": {},
   "source": [
    "# Document Classification with Supervised Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c2fe2",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f392dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, make_scorer, precision_score, recall_score, precision_recall_fscore_support, f1_score \n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384e564",
   "metadata": {},
   "source": [
    "### Importing and Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49742410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2    390\n",
      "1    210\n",
      "Name: doc_topics, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"SAMPLED_SPEECHES_3.csv\")\n",
    "df = df.dropna(subset=['doc_topics']) # Remove rows with NA values in the 'doc_topics' column\n",
    "\n",
    "X = df['speech_text']  # Features\n",
    "y = df['doc_topics']   # Classification label\n",
    "\n",
    "# Number of documents in each topic\n",
    "category_counts = df['doc_topics'].value_counts()\n",
    "print(category_counts)\n",
    "\n",
    "# Split the data - 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fbeee",
   "metadata": {},
   "source": [
    "### Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aeac8b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.1\n"
     ]
    }
   ],
   "source": [
    "# Count the number of instances in each class\n",
    "random.seed(1)\n",
    "N_hostility = sum(1 for label in y_train if label == 1)\n",
    "N_hospitality = sum(1 for label in y_train if label == 2)\n",
    "\n",
    "# Generate random predictions\n",
    "random_predictions = [random.choice([1, 2]) for doc_topics in range(len(y_train))]\n",
    "\n",
    "# Calculate accuracy based on random predictions\n",
    "accuracy_random = round(np.mean(np.array(random_predictions) == np.array(y_train)),3)*100\n",
    "print(accuracy_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba32b5f",
   "metadata": {},
   "source": [
    "### Pre-processing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc1bbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "custom_stopwords = [\"hon\", \"roseâ€”\", \"rose\", \"government\", \"minister\",\n",
    "                   \"gentleman\", \"speaker\", \"mr\", \"friend\", \"home\", \"secretary\",\n",
    "                   \"friend\", \"right\", \"<\", \">\", \"can\", \"lady\", \"people\"]\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]  # Remove non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove English stopwords\n",
    "    tokens = [token for token in tokens if token not in custom_stopwords]  # Remove custom stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens]  # Stemming\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to your text data for both training and testing sets\n",
    "X_train_preprocessed = [preprocess_text(text) for text in X_train]\n",
    "X_test_preprocessed = [preprocess_text(text) for text in X_test]\n",
    "\n",
    "# Defining feature representations (different max_feature value and feature types)\n",
    "vectorizers = [\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600),\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=900),\n",
    "    CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=1200),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=600),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=900),\n",
    "    CountVectorizer(min_df=5, lowercase=True, ngram_range=(1, 2), max_features=1200),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=600),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=900),\n",
    "    TfidfVectorizer(min_df=5, lowercase=True, max_features=1200)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ec2cac",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbf4a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      82.5      79.9       80.6    79.4   \n",
      "1      Unigram                 900      83.3      81.2       81.2    81.2   \n",
      "2      Unigram                1200      80.0      77.2       77.6    76.9   \n",
      "3       Bigram                 600      81.7      79.6       79.3    80.0   \n",
      "4       Bigram                 900      82.5      80.2       80.4    80.0   \n",
      "5       Bigram                1200      82.5      80.4       80.3    80.6   \n",
      "6       TF-IDF                 600      75.0      65.9       76.1    65.0   \n",
      "7       TF-IDF                 900      77.5      70.9       77.8    69.4   \n",
      "8       TF-IDF                1200      78.3      72.8       77.8    71.2   \n",
      "\n",
      "       Parameters  \n",
      "0  {'alpha': 0.7}  \n",
      "1  {'alpha': 0.3}  \n",
      "2  {'alpha': 0.5}  \n",
      "3  {'alpha': 0.7}  \n",
      "4  {'alpha': 0.5}  \n",
      "5  {'alpha': 0.7}  \n",
      "6  {'alpha': 0.1}  \n",
      "7  {'alpha': 0.1}  \n",
      "8  {'alpha': 0.1}  \n"
     ]
    }
   ],
   "source": [
    "# Creating a Multinomial Naive Bayes Cross Validation function\n",
    "param_grid = {\n",
    "    'alpha': [0.05,0.08,0.1,0.3,0.5,0.7,1.0],  # Smoothing parameter (alpha)\n",
    "}\n",
    "\n",
    "def MultinomialNB_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(MultinomialNB(), param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_nb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_nb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "\n",
    "results = []\n",
    "        \n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, chosen_parameters = MultinomialNB_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid)\n",
    "    \n",
    "    # appending the results\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"nb_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2523ff29",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes on Training Set (Testing for Overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69fd1865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      85.8      85.0       84.3    86.2   \n",
      "1      Unigram                 900      88.5      88.0       87.2    89.9   \n",
      "2      Unigram                1200      89.8      89.2       88.4    90.8   \n",
      "3       Bigram                 600      84.8      83.9       83.2    85.0   \n",
      "4       Bigram                 900      87.7      87.0       86.3    88.6   \n",
      "5       Bigram                1200      89.6      89.0       88.2    90.7   \n",
      "6       TF-IDF                 600      89.2      87.7       89.9    86.3   \n",
      "7       TF-IDF                 900      91.7      90.6       92.3    89.4   \n",
      "8       TF-IDF                1200      94.8      94.2       95.1    93.4   \n",
      "\n",
      "       Parameters  \n",
      "0  {'alpha': 0.7}  \n",
      "1  {'alpha': 0.3}  \n",
      "2  {'alpha': 0.5}  \n",
      "3  {'alpha': 0.7}  \n",
      "4  {'alpha': 0.5}  \n",
      "5  {'alpha': 0.7}  \n",
      "6  {'alpha': 0.1}  \n",
      "7  {'alpha': 0.1}  \n",
      "8  {'alpha': 0.1}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    # Call the Multinomial Naive Bayes training and evaluation function on the training set\n",
    "    accuracy, f1, precision, recall, chosen_parameters = MultinomialNB_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train, param_grid)\n",
    "    \n",
    "    # Append the results to the results list\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy, 3)*100,\n",
    "        \"F1 Score\": round(f1, 3)*100,\n",
    "        \"Precision\": round(precision, 3)*100,\n",
    "        \"Recall\": round(recall, 3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"nb_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce925fd5",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4345fa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      80.0      73.9       82.5    71.9   \n",
      "1      Unigram                 900      80.8      74.7       84.6    72.5   \n",
      "2      Unigram                1200      80.8      74.7       84.6    72.5   \n",
      "3       Bigram                 600      79.2      72.5       81.7    70.6   \n",
      "4       Bigram                 900      78.3      71.1       81.0    69.4   \n",
      "5       Bigram                1200      80.0      73.3       84.0    71.2   \n",
      "6       TF-IDF                 600      76.7      65.6       87.0    65.0   \n",
      "7       TF-IDF                 900      76.7      65.6       87.0    65.0   \n",
      "8       TF-IDF                1200      76.7      66.5       83.7    65.6   \n",
      "\n",
      "      Parameters  \n",
      "0  {'C': [0.05]}  \n",
      "1  {'C': [0.04]}  \n",
      "2  {'C': [0.04]}  \n",
      "3  {'C': [0.05]}  \n",
      "4  {'C': [0.04]}  \n",
      "5  {'C': [0.04]}  \n",
      "6   {'C': [1.0]}  \n",
      "7   {'C': [1.0]}  \n",
      "8   {'C': [1.0]}  \n"
     ]
    }
   ],
   "source": [
    "def logit_train_and_evaluate(X_train, X_test, y_train, y_test, custom_Cs=None):\n",
    "    if custom_Cs is None:\n",
    "        custom_Cs = [0.01, 0.02, 0.03,0.04,0.05,1.0]\n",
    "\n",
    "    lasso_logit_model = LogisticRegressionCV(Cs=custom_Cs, scoring=\"f1\")  \n",
    "    lasso_logit_model.fit(X_train, y_train) \n",
    "    y_pred = lasso_logit_model.predict(X_test)  \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro',zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    chosen_parameters = {\n",
    "        \"C\": lasso_logit_model.C_,\n",
    "    }\n",
    "\n",
    "    return accuracy, f1, precision, recall, conf_matrix, chosen_parameters\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\" \n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, conf_matrix, chosen_parameters = logit_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"lr_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23098c6",
   "metadata": {},
   "source": [
    "### Logistic Regression on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfc590b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy (Train)  F1 Score (Train)  \\\n",
      "0      Unigram                 600              92.3              91.1   \n",
      "1      Unigram                 900              91.7              90.3   \n",
      "2      Unigram                1200              91.9              90.6   \n",
      "3       Bigram                 600              92.3              91.1   \n",
      "4       Bigram                 900              91.9              90.6   \n",
      "5       Bigram                1200              92.3              91.1   \n",
      "6       TF-IDF                 600              89.4              87.5   \n",
      "7       TF-IDF                 900              89.4              87.4   \n",
      "8       TF-IDF                1200              89.0              86.9   \n",
      "\n",
      "   Precision (Train)  Recall (Train) Parameters (Train)  \n",
      "0               94.7            89.1      {'C': [0.05]}  \n",
      "1               94.3            88.2      {'C': [0.04]}  \n",
      "2               94.4            88.5      {'C': [0.04]}  \n",
      "3               94.2            89.4      {'C': [0.05]}  \n",
      "4               94.4            88.5      {'C': [0.04]}  \n",
      "5               94.7            89.1      {'C': [0.04]}  \n",
      "6               92.3            85.3       {'C': [1.0]}  \n",
      "7               92.9            85.0       {'C': [1.0]}  \n",
      "8               92.4            84.5       {'C': [1.0]}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\" \n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy_train, f1_train, precision_train, recall_train, conf_matrix_train, chosen_parameters_train = logit_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train)\n",
    "\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy (Train)\": round(accuracy_train, 3) * 100,\n",
    "        \"F1 Score (Train)\": round(f1_train, 3) * 100,\n",
    "        \"Precision (Train)\": round(precision_train, 3) * 100,\n",
    "        \"Recall (Train)\": round(recall_train, 3) * 100,\n",
    "        \"Parameters (Train)\": chosen_parameters_train,\n",
    "    })\n",
    "\n",
    "# Print a dataframe from the results list\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"lr_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f581d85d",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfae5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy  F1 Score  Precision  Recall  \\\n",
      "0      Unigram                 600      79.2      71.9       83.4    70.0   \n",
      "1      Unigram                 900      80.0      73.3       84.0    71.2   \n",
      "2      Unigram                1200      79.2      71.9       83.4    70.0   \n",
      "3       Bigram                 600      78.3      70.4       82.7    68.8   \n",
      "4       Bigram                 900      79.2      71.9       83.4    70.0   \n",
      "5       Bigram                1200      80.8      74.7       84.6    72.5   \n",
      "6       TF-IDF                 600      76.7      70.7       75.4    69.4   \n",
      "7       TF-IDF                 900      79.2      73.6       79.5    71.9   \n",
      "8       TF-IDF                1200      78.3      71.7       79.7    70.0   \n",
      "\n",
      "                        Parameters  \n",
      "0  {'kernel': 'linear', 'nu': 0.6}  \n",
      "1  {'kernel': 'linear', 'nu': 0.6}  \n",
      "2  {'kernel': 'linear', 'nu': 0.6}  \n",
      "3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "4  {'kernel': 'linear', 'nu': 0.6}  \n",
      "5  {'kernel': 'linear', 'nu': 0.6}  \n",
      "6  {'kernel': 'linear', 'nu': 0.6}  \n",
      "7  {'kernel': 'linear', 'nu': 0.6}  \n",
      "8  {'kernel': 'linear', 'nu': 0.6}  \n"
     ]
    }
   ],
   "source": [
    "# Define the SVM parameter grid\n",
    "param_grid = {\n",
    "    'nu': [0.6,0.65,0.7],  # nu parameter\n",
    "    'kernel': ['linear'],  # kernel functions\n",
    "}\n",
    "\n",
    "def SVM_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(NuSVC(), param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_nusvm_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_nusvm_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    # Call the SVM training and evaluation function\n",
    "    accuracy, f1, precision, recall, chosen_parameters = SVM_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid)\n",
    "    \n",
    "    # Append the results to the results list\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy,3)*100,\n",
    "        \"F1 Score\": round(f1,3)*100,\n",
    "        \"Precision\": round(precision,3)*100,\n",
    "        \"Recall\": round(recall,3)*100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"svm_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432e4905",
   "metadata": {},
   "source": [
    "### Support Vector Machine on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5edf8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature Type  Number of Features  Accuracy (Train)  F1 Score (Train)  \\\n",
      "0      Unigram                 600              89.4              87.4   \n",
      "1      Unigram                 900              90.2              88.5   \n",
      "2      Unigram                1200              90.6              89.0   \n",
      "3       Bigram                 600              88.8              86.6   \n",
      "4       Bigram                 900              89.6              87.6   \n",
      "5       Bigram                1200              90.4              88.7   \n",
      "6       TF-IDF                 600              92.3              91.3   \n",
      "7       TF-IDF                 900              94.2              93.4   \n",
      "8       TF-IDF                1200              96.0              95.6   \n",
      "\n",
      "   Precision (Train)  Recall (Train)               Parameters (Train)  \n",
      "0               92.6            85.1  {'kernel': 'linear', 'nu': 0.6}  \n",
      "1               93.1            86.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "2               93.7            86.8  {'kernel': 'linear', 'nu': 0.6}  \n",
      "3               92.3            84.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "4               93.1            85.3  {'kernel': 'linear', 'nu': 0.6}  \n",
      "5               93.5            86.5  {'kernel': 'linear', 'nu': 0.6}  \n",
      "6               93.3            89.9  {'kernel': 'linear', 'nu': 0.6}  \n",
      "7               95.4            92.0  {'kernel': 'linear', 'nu': 0.6}  \n",
      "8               96.9            94.5  {'kernel': 'linear', 'nu': 0.6}  \n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    # Call the SVM training and evaluation function on the training set\n",
    "    accuracy_train, f1_train, precision_train, recall_train, chosen_parameters_train = SVM_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train, param_grid)\n",
    "\n",
    "    # Append the results to the results list\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy (Train)\": round(accuracy_train, 3) * 100,\n",
    "        \"F1 Score (Train)\": round(f1_train, 3) * 100,\n",
    "        \"Precision (Train)\": round(precision_train, 3) * 100,\n",
    "        \"Recall (Train)\": round(recall_train, 3) * 100,\n",
    "        \"Parameters (Train)\": chosen_parameters_train\n",
    "    })\n",
    "\n",
    "# Print a dataframe from the results list\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"svm_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd1b595",
   "metadata": {},
   "source": [
    "## Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b602df",
   "metadata": {},
   "source": [
    "### Making Word Vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7d4bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Load the Word2Vec model\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "\n",
    "# Encode documents using Word2Vec\n",
    "def encode_document(document, word2vec_model):\n",
    "    # Tokenize the document into words\n",
    "    words = document.split()\n",
    "\n",
    "    # Encode each word using the pre-trained Word2Vec model\n",
    "    word_vectors = [word2vec_model[word] for word in words if word in word2vec_model]\n",
    "\n",
    "    return word_vectors\n",
    "\n",
    "# Aggregate word vectors for document representation\n",
    "def average_word_vectors(word_vectors):\n",
    "    if not word_vectors:\n",
    "        return np.zeros(word2vec_model.vector_size)  # Return zeros if no word vectors are available\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Encode training and testing documents\n",
    "X_train_encoded = [encode_document(doc, word2vec_model) for doc in X_train_preprocessed]\n",
    "X_test_encoded = [encode_document(doc, word2vec_model) for doc in X_test_preprocessed]\n",
    "\n",
    "# Aggregate word vectors for document representation\n",
    "X_train_avg = [average_word_vectors(doc) for doc in X_train_encoded]\n",
    "X_test_avg = [average_word_vectors(doc) for doc in X_test_encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35986e5",
   "metadata": {},
   "source": [
    "### Logistic Regression with Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f674695d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.8\n",
      "F1 Score: 50.8\n",
      "Precision: 78.9\n",
      "Recall: 37.5\n",
      "Best Hyperparameter (C): [5.]\n",
      " \n",
      "Testing for Overfitting on the Training Set\n",
      "Accuracy: 83.3\n",
      "F1 Score: 71.8\n",
      "Precision: 89.5\n",
      "Recall: 60.0\n",
      "Best Hyperparameter (C): [5.]\n"
     ]
    }
   ],
   "source": [
    "custom_Cs = [0.01, 0.03, 0.05, 1, 5]\n",
    "\n",
    "# Logistic Regression with cross-validated hyperparameter search\n",
    "logit_model = LogisticRegressionCV(\n",
    "    Cs=custom_Cs,\n",
    "    cv=10,            # Number of cross-validation folds\n",
    "    random_state=123, # Set a random seed for reproducibility\n",
    "    max_iter=1000,     # max_iter is set at 1000 to ensure the model converges\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "# Training\n",
    "logit_model.fit(X_train_avg, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = logit_model.predict(X_test_avg)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "accuracy = accuracy_score(y_test, y_pred) # accuracy\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_C = logit_model.C_\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score* 100:.1f}\")\n",
    "print(f\"Precision: {precision* 100:.1f}\")\n",
    "print(f\"Recall: {recall* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")\n",
    "\n",
    "\n",
    "# TESTING FOR OVERFITTING\n",
    "y_pred_train = logit_model.predict(X_train_avg)\n",
    "\n",
    "# Calculate precision, recall, and F1 score on the training set\n",
    "accuracy = accuracy_score(y_train, y_pred_train)  # accuracy\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_train, y_pred_train, average='binary')\n",
    "\n",
    "# Print results\n",
    "print(\" \")\n",
    "print(\"Testing for Overfitting on the Training Set\")\n",
    "print(f\"Accuracy: {accuracy * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score * 100:.1f}\")\n",
    "print(f\"Precision: {precision * 100:.1f}\")\n",
    "print(f\"Recall: {recall * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (C): {best_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f45162",
   "metadata": {},
   "source": [
    "### SVM with Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "537bed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.5\n",
      "F1 Score: 60.9\n",
      "Precision: 72.4\n",
      "Recall: 52.5\n",
      "Best Hyperparameter (nu): 0.1\n",
      "Best Hyperparameter (kernel): rbf\n",
      " \n",
      "Results on the Training Set:\n",
      "Accuracy: 100.0\n",
      "F1 Score: 100.0\n",
      "Precision: 100.0\n",
      "Recall: 100.0\n",
      "Best Hyperparameter (nu): 0.1\n",
      "Best Hyperparameter (kernel): rbf\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'nu': [0.1,0.2,0.3],  # nu parameter\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # kernel functions\n",
    "}\n",
    "\n",
    "# NuSVM with grid search\n",
    "nu_svm_model = NuSVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(nu_svm_model, param_grid, cv=10,scoring=\"f1\")\n",
    "grid_search.fit(X_train_avg, y_train)\n",
    "\n",
    "# Access the best hyperparameters\n",
    "best_nu = grid_search.best_params_['nu']\n",
    "best_kernel = grid_search.best_params_['kernel']\n",
    "\n",
    "# Predictions with the best model\n",
    "y_pred_nu_svm = grid_search.predict(X_test_avg)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "accuracy_nu_svm = accuracy_score(y_test, y_pred_nu_svm)\n",
    "precision_nu_svm, recall_nu_svm, f1_score_nu_svm, _ = precision_recall_fscore_support(y_test, y_pred_nu_svm, average='binary')\n",
    "\n",
    "# Print results\n",
    "print(f\"Accuracy: {accuracy_nu_svm* 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_nu_svm* 100:.1f}\")\n",
    "print(f\"Precision: {precision_nu_svm* 100:.1f}\")\n",
    "print(f\"Recall: {recall_nu_svm* 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (nu): {best_nu}\")\n",
    "print(f\"Best Hyperparameter (kernel): {best_kernel}\")\n",
    "\n",
    "# TESTING FOR OVERFITTING\n",
    "y_pred_nu_svm_train = grid_search.predict(X_train_avg)\n",
    "\n",
    "# Calculate precision, recall, and F1 score on the training set\n",
    "accuracy_nu_svm_train = accuracy_score(y_train, y_pred_nu_svm_train)\n",
    "precision_nu_svm_train, recall_nu_svm_train, f1_score_nu_svm_train, _ = precision_recall_fscore_support(\n",
    "    y_train, y_pred_nu_svm_train, average='binary'\n",
    ")\n",
    "\n",
    "# Print results for the training set\n",
    "print(\" \")\n",
    "print(\"Results on the Training Set:\")\n",
    "print(f\"Accuracy: {accuracy_nu_svm_train * 100:.1f}\")\n",
    "print(f\"F1 Score: {f1_score_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Precision: {precision_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Recall: {recall_nu_svm_train * 100:.1f}\")\n",
    "print(f\"Best Hyperparameter (nu): {best_nu}\")\n",
    "print(f\"Best Hyperparameter (kernel): {best_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde25356",
   "metadata": {},
   "source": [
    "## Chosen Model: Naive Bayes, Unigram, 900 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed7b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf = pd.read_csv(\"debate_dataset.csv\")\n",
    "fullX = fulldf['speech_text']  # Features\n",
    "fullX_preprocessed = [preprocess_text(text) for text in fullX]\n",
    "\n",
    "# Create and fit the vectorizer on your training data first\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=900)\n",
    "X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "\n",
    "# Transform the full dataset\n",
    "X_full = vectorizer.transform(fullX_preprocessed)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB(alpha=0.7)\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "\n",
    "y_pred_full = naive_bayes_classifier.predict(X_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96756426",
   "metadata": {},
   "source": [
    "## Labelling the rest of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b4c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fulldf['predicted_label'] = y_pred_full\n",
    "fulldf.to_csv(\"fulldf_with_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e109721",
   "metadata": {},
   "source": [
    "## Evaluating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d5182",
   "metadata": {},
   "source": [
    "#### Printing Correctly Classified Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "137836a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly classified documents:\n",
      "Document 480:\n",
      "last time shall interven want interrupt flow last point accept offer put extra ground greec includ specialist abil support greek demonstr fact uk play role support greec thing call\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 481:\n",
      "hope acknowledg discuss possibl futur visa liberalis involv schengen countri involv eu member state part schengen\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 482:\n",
      "congratul member north thanet sir roger gale secur urgent question point lack consult local author health servic frankli appal want ask number question report parliamentari group immigr detent call end accommod asylum seeker describ fundament unsuit survivor war tortur seriou violenc offic ignor warn parliamentari colleagu offic previous ignor warn use napier barrack red cross public health england inevit result covid outbreak among held pandem enter anoth danger phase commit listen expert time follow health guidanc confirm part manston estat current condemn result asbesto found site know littl fact none local author key partner health servic tell us consult taken place organis work tortur survivor victim traffick trauma napier lack proper plan process final mention illeg migrant depart commit end languag thing illeg migrant seek asylum illeg put end languag pander lowest common denomin\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 483:\n",
      "ultim need bear asylum backlog fewer uk wait decis also need stop use hotel million night\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 484:\n",
      "affair committe report channel cross migrat asylum show take averag day process unaccompani children report also illustr unaccompani children go miss hotel sometim temporarili sometim perman find children protect crimin sexual exploit\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 486:\n",
      "condit manston clearli unsaf inhuman know suffer experienc year shame watch howev also understand lack accommod across countri open home refuge scheme support properli commun\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 487:\n",
      "make fair enough point offic still make eu settlement scheme access possibl return point due cours\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 488:\n",
      "learn make import point inde discuss european union level colleagu particularli french interior encourag european commiss work pace initi propos centr niger look european commiss possibl centr east africa well obvious import look care appropri centr need place safeti individu also relat import issu illeg econom migrant rather refuge break link make peril journey across mediterranean gain settlement europ\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 489:\n",
      "form lack compass toward fled horror imagin abandon dub child refuge scheme broken system leav asylum seeker limbo month possibl year go food bank even minim support entitl often arriv constitu faith whether one iota compass\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 490:\n",
      "thank give way point citizen uk identifi live case put system say fact accept half actual made britain brought straight away still stuck calai cold live tent mud huge risk accept famili care\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 491:\n",
      "assum refer detent centr know detent polici cover bill ask import question want make sure answer polici make absolut clear detent last resort respect immigr control individu subject remov manag know would approv anyon detain absolut requir must certain reason prospect remov reason time despit protect also tri make sure welcom work done independ shaw report tri time see improv polici\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 492:\n",
      "last polit settlement syria key address tragic issu meanwhil mani strand struggl greec greek island welcom commit look way help migrant situat find meanwhil lot aid agenc local agenc constitu hope aid direct volunt go greek island help join commend wish mani help time need\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 495:\n",
      "constitu larkhal suffer psycholog emot financi abus husband ran busi seven year uk held spousal visa due length time took approv indefinit leav remain choic remain marriag leav uk due offic bureaucraci accept offic need sensit case\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 496:\n",
      "import recognis like deal two distinct group first extend famili within uk alreadi famili pressur might otherwis foster differ relat children resettl basi risk exploit abus need discuss detail local well differ govern engag fund link need hold convers offic alreadi provid fund unaccompani children assur learn talk scottish local author scotland well england wale northern ireland seen contribut make unit kingdom\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 498:\n",
      "across scotland common europhil part uk huge upsurg applic indefinit remain mr fabiola power spanish birth got marri resid acton decad dismay reject prove five year continu servic employ paid privat health insur revisit rigid requir penalis eu nation mr power homemak student contract end bureaucrat nightmar\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 501:\n",
      "answer member oldham east saddleworth debbi abraham basic said local author corpor parent respons children hotel case afraid offic packag local woefulli inadequ know day state hous commun local current depart duti protect children refuge tell hous today mani nation crime agenc offic work seriou issu child traffick disappear hotel\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 502:\n",
      "save children estim unaccompani children live refuge camp calai dunkirk mani famili uk howev famili reunif process french uk take month result mani children make danger decis cross channel tell hous process take long step taken speed\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 503:\n",
      "confirm uk fact invad recognis respons rather humanitarian respons term clandestin channel threat command fuel tension scapego asylum seeker racism\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 504:\n",
      "say behov member hous care speak relat matter job respond question respond question\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 505:\n",
      "rais import point speak heroic border forc offici regular basi tell experi seen repeat vexati claim see arriv make claim might see lawyer suddenli come claim seen lawyer make repeat late claim well know game system real concern practic lawyer encourag author take action\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 506:\n",
      "take action fix asylum system firm system abus fair need protect clear use everi mean dispos make use small boat cross channel unviabl last week laid chang immigr rule vital curb irregular migrat often facilit ruthless crimin gang channel cross highli danger unnecessari franc european countri safe asylum claim chang mean individu could claim asylum previous safe countri may asylum claim determin uk abl safe return chang also enabl us consid return individu safe countri besid safe countri could claim asylum individu also abl make asylum claim sea end transit period uk longer bound dublin regul new measur enabl us agreement replac dublin flexibl return arrang deterr effect send clear messag anyon think come uk danger safe countri risk live deterr effect also destroy busi model ruthless crimin gang return would cours reduc number accommod want clear turn back need help flee persecut oppress tyranni stand oblig refuge convent european convent human right relev treati continu welcom uk safe legal rout assist vulner provid accommod meet essenti live need set take number step tackl irregular danger migrat address problem realli requir complet overhaul first half next year bring forward bill fix immigr asylum system countri fair need protect firm system abus\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 507:\n",
      "fail labour opposit member fail stand british fail support measur stop boat want open border unlimit migrat\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 508:\n",
      "agre narr immigr need chang heard asylum seeker flee persecut greater number risk live cross channel flimsi boat net neg immigr leav countri arriv agre proportion uk support lower number germani spain greec franc\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 509:\n",
      "rise speak support amend lieu lord amend also lord amend pregnant women also want touch lord amend oppos southend west want tough fair polici illeg immigr stop unfairli jump queue stop evil smuggler stop vulner drown channel nutshel reason support bill subject amend allud reject entir characteris heard opposit member way ungener countri believ take pride uk rich histori rehom world vulner persecut remind us taken refuge around globe sinc highest number sinc second world ukrainian southend west think generos humanitarian spirit extend healthi young men safe countri paid smuggler help illeg cross channel bill remind us three quarter cross channel last year men age number indian nation cross uk small boat increas india democrat countri space programm turkish citizen came countri illeg last year turkey safe nato countri almost million british nation year go holiday reason whatsoev indian turk come illeg scheme would welcom come legal cours deepli unfair taxpay come countri legal put point perspect huge major promis level countri yet bizarr situat spend year hotel accommod illeg billion accord shadow entir budget round level fund three half time spend homeless unsustain deepli unfair vulner countri need support heard said emerg absolut becom emerg lord hagu wrote africa middl east time today one twenti region migrat conserv would million would complet emerg migrat system must send clear messag allow come illeg lord sent us ream amend consid mani design frustrat put us kick issu road lead directli lord amend name archbishop canterburi strategi fine well need action see televis screen drown channel cours work hard strategi countri europ face problem henc work franc itali albania eu reject approach suggest lord amend alreadi happen address emerg us howev pleas accept amend particularli retrospect strong presumpt common law statut take retrospect effect recent summaris lord kerr suprem court walker innospec limit other someth law appli law forc today tomorrow backward adjust word retrospect law undermin rule law requir law capabl known enforc today understand thought retrospect effect need reason appli rwanda issu need resolv suprem court therefor pleas sensibl amend taken board fall line legal principl second issu relat pregnant women obvious protect vital must everyth ensur vulner pregnant women exploit target evil smuggler last year fewer illeg migrant came countri pregnant understand year figur none incred care creat pervers incent might inadvert increas number must extrem awar would traffic women countri utterli without moral want find posit women becom pregnant deliber even wors made pregnant order bypass detent rule amend allow us protect vulner pregnant women ensur spend unduli long period detent process strike balanc treat women digniti compass creat pervers incent would target vulner women conclus although compass seek help may infinit southend know capac finit capac help fundament undermin stop boat stop enter countri illeg\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 511:\n",
      "togeth colleagu depart level hous commun provid rang support access public servic includ essenti school place children hous around move process move new home sinc first arap flight june unpreced rate resettl\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 512:\n",
      "much discuss europ turn work effect turkey worth point turkey around half refuge left syria date million refuge highlight work turkey work help humanitarian support work describ europ broadli relat registr help countri europ process refuge arriv shore\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 514:\n",
      "plaid cymru reject cynic frame issu crisi number true crisi lack human success westminst govern scapegoat legitim asylum seeker refuge wale take differ approach state ambit nation sanctuari seek support flee persecut sadli asylum remain reserv matter consider given lack compat approach channel cross wale nation sanctuari asylum seeker plan\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 515:\n",
      "join commend work cutter hmc protector hmc seeker year rescu play part apprehens fewer traffick explain hous cutter withdrawn servic time given clearli see drop number come across mediterranean seen previou year around time\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 516:\n",
      "former raf barrack manston releas ministri defenc requir site clear local hous lie adjac manston airport hope see reopen near futur airfield develop consent order determin appropri site propos purpos friday decemb receiv email execut offic kent wing inform air train unit given today decemb vacat premis former raf barrack fire train school order immigr centr could establish describ us debat order us first heard propos consult member parliament leader counti council leader thanet district council believ counti constabulari spoke state day promis full brief civil servant present leader thanet district council call offic offici pm monday two day ago leader kent counti council pm consult date kent senior health offic consult even inform offici offic known develop traffick issu month develop crisi week propos creat screen process centr unsuit manston road site neither propos phase transfer triag facil tug manston discuss propos phase expans facil handl number migrant unspecifi length time consult told civil servant lead project understand work visit site offic establish process might christma met state yesterday ask stop put project proper consult facilit degre courtesi offic signal lack date report phone call made last night clear offici ignor request blunder request urgent question appear state blindsid offici yet anoth reaction problem ought foreseen avoid stand current propos appear transfer arriv secur tug manston barrack accommod marque detain secur process indic site accommod human be made secur facil made avail statutori medic servic real subject great miseri result circumst may discuss anoth occas result lack foresight prepar propos process larg unsuit condit simpli satisfi perceiv demand met mean identifi least one clean comfort secur oper vessel commiss meet immedi longer term need advis other avail would grate would instruct team alreadi request put unaccept unwork propos hold properli thoroughli swiftli examin viabl altern perhap could conduct consult ought held week ago tri railroad bad idea shelter christma recess unfortun undesir consequ commun affect\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 517:\n",
      "want get polit particular issu neither seemli appropri term rout say place first prioriti famili rel appreci vast major rel abl stay famili work pace set humanitarian sponsorship visa obvious colleagu depart level hous commun lead operationalis actual sponsorship element ensur give welcom mani want see\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 518:\n",
      "may ask put record tribut bbc journalist report around world highlight us great sensit great honesti appal conflict famin plight refuge cruelti experienc christian minor also pay tribut journalist given live report issu around world perhap urg someth un protect\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 519:\n",
      "agre oxford professor constitut law richard ekin wrote sunday root problem human right act incorpor european convent human right law enabl court interpret legisl unreason contradict revisit legisl matter decid unelect judg strasbourg\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 520:\n",
      "said lawyer duti confidenti client simpli permit tell inde anybodi els legal advic share offic use human right act section b statement mean bill breach echr mean state bill like compat convent right legal challeng made take step defend posit court\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 521:\n",
      "recognis challeng mani local author face deal particularli vulner children increas fund around hope help find resourc need deal particular children\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 522:\n",
      "welcom statement state explain electron travel authoris scheme work common travel area especi visitor arriv dublin travel uk would visitor need get authoris situat\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 523:\n",
      "minut ago appear confirm consid afghan genuin need report afghan plan flight report correct\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 524:\n",
      "syrian crisi creat nearli million refuge yet fewer settl back uk syrian vulner person reloc scheme given need safe passag seek asylum countri say discuss counterpart offic discuss expand number safeti seek asylum uk\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 527:\n",
      "congratul secur debat agre organis involv includ council nation chariti local bookshop also local songwork choir local school make collect children hornsey wood green inspir collect food blanket give pocket money church mosqu across piec also involv somebodi came advic surgeri friday refuge thought want discuss hous problem rais local chariti call comkar tear photograph littl boy said journey made want bit could also reach help civic commun\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 528:\n",
      "ask think quit deepli issu poor taken harwich tip iceberg hundr thousand around world victim war oppress human right abus appar mani come afghanistan occupi past year think worldwid humanitarian crisi address save live fine condemn traffick must look consequ desper poor\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 531:\n",
      "lost gener initi mean fulli back lebanon reach children educ plan fund educ lebanon year increas million million support lebanes govern effort doubl number syrian children enrol lebanes public school\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 532:\n",
      "certainli pictur press children blanket head specif protect ident children ident need protect confid compass british wish support us small minor media nois listen\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 533:\n",
      "wish hous sure depart listen care extrem concern situat mani month regular contact state foreign commonwealth affair member rochford southend east jame duddridg respons africa diplomat front term plan conting event escal humanitarian crisi\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 534:\n",
      "would happi attend committe think date septemb set although sure whether share collabor work french\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 535:\n",
      "reason amend name leader opposit select member see great deal interest debat first speech come six minut everyon els could start think term four three minut would help call state move motion second read\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 537:\n",
      "make excel point make sure offic look thing automat suggest might happen review afresh make sure still fit purpos\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 538:\n",
      "met directli sinc appoint work close state depart member corbi tom pursglov respons immigr close align issu\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 539:\n",
      "bbc news websit report past minut court appeal decid block flight rwanda week rememb nation border bill committe consid support allow process asylum claim safe third decis repeatedli restat whole hous consid bill decid whether rwanda safe countri awar countri intern organis make use resettl rwanda either asylum seeker refuge\n",
      "Predicted Label: 1\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 540:\n",
      "tell hous mani increas made british includ british children year made largest group increas talk british children also point hous exactli make referr human traffick system countri fact done auspic offic claim actual offic say whether\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 541:\n",
      "last week uk committe climat chang warn track miss intern target reduc emiss unless take urgent action effect climat chang felt acut develop countri caus even hardship suffer seek discuss intern take action\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 542:\n",
      "hope understand comment particular case dispatch box destitut domest violenc give immedi crisi support victim domest abus whose resid statu depend partner may well abus\n",
      "Predicted Label: 2\n",
      "Actual Label: 2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizers = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600)\n",
    "X_train_dtm = vectorizers.fit_transform(X_train_preprocessed)\n",
    "X_test_dtm = vectorizers.transform(X_test_preprocessed)\n",
    "\n",
    "naive_bayes_classifier = MultinomialNB(alpha=0.7)\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "x_pred_chosenmodel = naive_bayes_classifier.predict(X_test_dtm)\n",
    "\n",
    "# Printing correctly classified documents\n",
    "correct_indices = [i for i, (pred, actual) in enumerate(zip(x_pred_chosenmodel, y_test)) if pred == actual]\n",
    "\n",
    "print(\"Correctly classified documents:\")\n",
    "for idx in correct_indices[:50]: \n",
    "    print(f\"Document {idx + len(X_train)}:\")\n",
    "    print(X_test_preprocessed[idx])  \n",
    "    print(\"Predicted Label:\", x_pred_chosenmodel[idx]) \n",
    "    print(\"Actual Label:\", y_test.iloc[idx])  \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a7c7986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 30 features for class 1:\n",
      "['countri', 'asylum', 'would', 'immigr', 'member', 'illeg', 'say', 'make', 'come', 'system', 'take', 'want', 'work', 'issu', 'point', 'bill', 'border', 'need', 'go', 'year', 'said', 'mani', 'import', 'way', 'know', 'european', 'migrat', 'time', 'made', 'number']\n",
      "Top 30 features for class 2:\n",
      "['countri', 'refuge', 'member', 'work', 'children', 'asylum', 'support', 'make', 'need', 'would', 'uk', 'mani', 'come', 'one', 'year', 'take', 'famili', 'hous', 'offic', 'local', 'say', 'help', 'us', 'bill', 'also', 'respons', 'know', 'go', 'system', 'point']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.95, lowercase=True, max_features=600)\n",
    "X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "\n",
    "# Train Multinomial Naive Bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB(alpha=0.7)\n",
    "naive_bayes_classifier.fit(X_train_dtm, y_train)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get the log probabilities of features given each class\n",
    "log_probabilities = naive_bayes_classifier.feature_log_prob_\n",
    "\n",
    "# Find the most important terms for each class\n",
    "top_n = 30\n",
    "for i, target_class in enumerate(naive_bayes_classifier.classes_):\n",
    "    print(f\"Top {top_n} features for class {target_class}:\")\n",
    "    class_probabilities = log_probabilities[i]  # Log probabilities for current class\n",
    "    top_indices = class_probabilities.argsort()[-top_n:][::-1]  # Indices of top features\n",
    "    top_features = [feature_names[index] for index in top_indices]  # Top features\n",
    "    print(top_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a32938",
   "metadata": {},
   "source": [
    "#### Printing Incorrectly Classified Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2274cbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incorrectly classified documents:\n",
      "Document 485:\n",
      "welcom statement agre whole hous prioriti prevent cross first place welcom said increas surveil air sea land franc nevertheless morn vessel left franc land dung constitu know often particularli treacher part coast need done prevent boat slip net case vessel detect local fish craft alert author first spot author\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 493:\n",
      "much made safeguard illeg migrant think member hous would agre talk safeguard citizen thousand come know background forc put hotel nowher els go guarante give citizen live near hotel safe particularli hear go hotel\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 494:\n",
      "happi give assur rightli say need chang time immigr system built futur flexibl meet need economi societi give one exampl look worker scheme look mani need economi includ cornwal need hospit industri season natur much demand happili give assur\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 497:\n",
      "point clarif reason lot question opposit urgent would like ask whether serious consid use sonic boom weaponri seek come countri alreadi hazard condit channel pleas explain hous impact weaponri use individu may also say oper red meat oper save big want call name\n",
      "Predicted Label: 1\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 499:\n",
      "absolut thank comment sum total challeng confront us exist long time exactli attempt fix broken system tackl issu gang sourc go upstream use intellig fix system unit kingdom cours continu work counterpart around world\n",
      "Predicted Label: 1\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 500:\n",
      "go make progress measur support bill requir public servic staff speak fluent english cours sensibl propos howev believ legisl matter respons bear mind time difficult sensit polici area unlik issu debat hous one potenti caus real harm strife commun support get balanc want clear support legisl introduc hast back clear evid problem bill part appear draft beer mat pub secretari speech conserv parti confer manchest legisl driven desir seen someth get headlin\n",
      "Predicted Label: 1\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 510:\n",
      "consciou need make progress time short give way\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 513:\n",
      "inform motherwel wishaw welcom refuge vietnames congoles syrian pleas make mistak econom assess say set annual cap reduc inflow enter uk therefor reduc cost associ process asylum claim secondari interrupt sorri madam deputi feel well\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n",
      "Document 525:\n",
      "alway believ duti meet intern oblig fair firm humanitarian way current system fail three import draw distinct legal migrat refuge victim traffick illeg immigr differ thing offic announc plan introduc tougher age assess confirm hous new assess sensit disabl trauma medic need well carri digniti respect\n",
      "Predicted Label: 1\n",
      "Actual Label: 2\n",
      "\n",
      "\n",
      "Document 526:\n",
      "rememb evid given mayor calai affair committe said public major camp inform need claim asylum franc want want come uk agre incumb french calai author ensur children make asylum applic assist adult inform must claim asylum franc safe countri\n",
      "Predicted Label: 2\n",
      "Actual Label: 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get indices of incorrectly classified documents\n",
    "incorrect_indices = [i for i, (pred, actual) in enumerate(zip(x_pred_chosenmodel, y_test)) if pred != actual]\n",
    "\n",
    "# Print 10 incorrectly classified documents\n",
    "print(\"Incorrectly classified documents:\")\n",
    "for idx in incorrect_indices[:10]:  \n",
    "    print(f\"Document {idx + len(X_train)}:\")\n",
    "    print(X_test_preprocessed[idx])  \n",
    "    print(\"Predicted Label:\", x_pred_chosenmodel[idx]) \n",
    "    print(\"Actual Label:\", y_test.iloc[idx]) \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcede5d",
   "metadata": {},
   "source": [
    "# Sensitivity Test - Additional Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546608a3",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c772fc64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CountVectorizer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accuracy, f1, precision, recall, best_params\n\u001b[1;32m     27\u001b[0m results_rf \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vectorizer \u001b[38;5;129;01min\u001b[39;00m vectorizers:\n\u001b[1;32m     30\u001b[0m     X_train_dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(X_train_preprocessed)\n\u001b[1;32m     31\u001b[0m     X_test_dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test_preprocessed)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'CountVectorizer' object is not iterable"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_score, recall_score\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Random Forest parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [30,50],\n",
    "    'max_depth': [5, 10, 20],\n",
    "}\n",
    "\n",
    "def RandomForest_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_rf_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_rf_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "results_rf = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    # Call the Random Forest training and evaluation function\n",
    "    accuracy_rf, f1_rf, precision_rf, recall_rf, chosen_params_rf = RandomForest_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid_rf)\n",
    "    \n",
    "    # Append the results to the results_rf list\n",
    "    results_rf.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy_rf, 3) * 100,\n",
    "        \"F1 Score\": round(f1_rf, 3) * 100,\n",
    "        \"Precision\": round(precision_rf, 3) * 100,\n",
    "        \"Recall\": round(recall_rf, 3) * 100,\n",
    "        \"Parameters\": chosen_params_rf\n",
    "    })\n",
    "\n",
    "results_rf_df = pd.DataFrame(results_rf)\n",
    "print(results_rf_df)\n",
    "\n",
    "results_rf_df.to_excel(\"rf_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae66fc",
   "metadata": {},
   "source": [
    "### Random Forest Classifier on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    # Call the SVM training and evaluation function on the training set\n",
    "    accuracy_train, f1_train, precision_train, recall_train, chosen_params_rf = RandomForest_train_and_evaluate(X_train_dtm, X_train_dtm, y_train, y_train, param_grid_rf)\n",
    "\n",
    "    # Append the results to the results list\n",
    "    results.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy (Train)\": round(accuracy_train, 3) * 100,\n",
    "        \"F1 Score (Train)\": round(f1_train, 3) * 100,\n",
    "        \"Precision (Train)\": round(precision_train, 3) * 100,\n",
    "        \"Recall (Train)\": round(recall_train, 3) * 100,\n",
    "        \"Parameters (Train)\": chosen_params_rf\n",
    "    })\n",
    "\n",
    "# Print a dataframe from the results list\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_excel(\"rf_training_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7215e8",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c68a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"SAMPLED_SPEECHES.csv\")\n",
    "df = df.dropna(subset=['doc_topics'])  # Remove rows with NA values in the 'doc_topics' column\n",
    "\n",
    "# Map labels: 2 -> 1, 1 -> 0\n",
    "df['doc_topics'] = df['doc_topics'].map({2: 1, 1: 0})\n",
    "\n",
    "# Split the data - 80% training and 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['speech_text'], df['doc_topics'], test_size=0.2, random_state=123)\n",
    "\n",
    "\n",
    "model = xgb.XGBClassifier(num_class=2)\n",
    "\n",
    "# Creating an XGBoost Cross Validation function\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [0.01],\n",
    "}\n",
    "\n",
    "\n",
    "def XGBoost_train_and_evaluate(X_train, X_test, y_train, y_test, param_grid):\n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False)\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=10, scoring='f1')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_xgb_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    y_pred = best_xgb_model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    return accuracy, f1, precision, recall, best_params\n",
    "\n",
    "# Results for XGBoost\n",
    "results_xgb = []\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    X_train_dtm = vectorizer.fit_transform(X_train_preprocessed)\n",
    "    X_test_dtm = vectorizer.transform(X_test_preprocessed)\n",
    "    num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "    if \"TfidfVectorizer\" in str(type(vectorizer)):\n",
    "        feature_type = \"TF-IDF\"\n",
    "    elif vectorizer.ngram_range == (1, 1):\n",
    "        feature_type = \"Unigram\"\n",
    "    else:\n",
    "        feature_type = \"Bigram\"\n",
    "\n",
    "    accuracy, f1, precision, recall, chosen_parameters = XGBoost_train_and_evaluate(X_train_dtm, X_test_dtm, y_train, y_test, param_grid_xgb)\n",
    "    \n",
    "    # appending the results\n",
    "    results_xgb.append({\n",
    "        \"Feature Type\": feature_type,\n",
    "        \"Number of Features\": num_features,\n",
    "        \"Accuracy\": round(accuracy, 3) * 100,\n",
    "        \"F1 Score\": round(f1, 3) * 100,\n",
    "        \"Precision\": round(precision, 3) * 100,\n",
    "        \"Recall\": round(recall, 3) * 100,\n",
    "        \"Parameters\": chosen_parameters\n",
    "    })\n",
    "\n",
    "results_xgb_df = pd.DataFrame(results_xgb)\n",
    "print(results_xgb_df)\n",
    "\n",
    "results_xgb_df.to_excel(\"xgb_results.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
